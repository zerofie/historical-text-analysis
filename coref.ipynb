{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fastcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade jupyter ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install jsonlib-bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(nltk.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\.opennre\n",
      "Path to rel2id file: C:\\Users\\Lenovo\\.opennre\\benchmark/wiki80/wiki80_rel2id.json\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Lenovo\\\\.opennre\\\\benchmark/wiki80/wiki80_rel2id.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mOpenNRE\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m opennre\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mopennre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwiki80_cnn_softmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\Documents\\m2\\OpenNRE\\opennre\\pretrain.py:144\u001b[0m, in \u001b[0;36mget_model\u001b[1;34m(model_name, root_path)\u001b[0m\n\u001b[0;32m    142\u001b[0m download(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglove\u001b[39m\u001b[38;5;124m'\u001b[39m, root_path\u001b[38;5;241m=\u001b[39mroot_path)\n\u001b[0;32m    143\u001b[0m download(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwiki80\u001b[39m\u001b[38;5;124m'\u001b[39m, root_path\u001b[38;5;241m=\u001b[39mroot_path)\n\u001b[1;32m--> 144\u001b[0m rel2id \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbenchmark/wiki80/wiki80_rel2id.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    145\u001b[0m sentence_encoder \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mCNNEncoder(token2id\u001b[38;5;241m=\u001b[39mwordi2d,\n\u001b[0;32m    146\u001b[0m                                              max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[0;32m    147\u001b[0m                                              word_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    153\u001b[0m                                              word2vec\u001b[38;5;241m=\u001b[39mword2vec,\n\u001b[0;32m    154\u001b[0m                                              dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    155\u001b[0m m \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mSoftmaxNN(sentence_encoder, \u001b[38;5;28mlen\u001b[39m(rel2id), rel2id)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Lenovo\\\\.opennre\\\\benchmark/wiki80/wiki80_rel2id.json'"
     ]
    }
   ],
   "source": [
    "from OpenNRE import opennre\n",
    "model = opennre.get_model('wiki80_cnn_softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (3825558422.py, line 174)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 174\u001b[1;36m\u001b[0m\n\u001b[1;33m    def abh_toh_maja_aayega(entities)\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import csv\n",
    "import itertools\n",
    "import wikipediaapi\n",
    "from typing import List,Tuple\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Doc, Span\n",
    "from fastcoref import FCoref\n",
    "# import wikipediaapi\n",
    "user_agent = \"MyCoolTool/1.0\"\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user_agent=user_agent, language='en', timeout=120)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "model = FCoref()\n",
    "\n",
    "def core_logic_part(document: Doc, coref: List[int], resolved: List[str], mention_span: Span):\n",
    "    final_token = document[coref[1]]\n",
    "    if final_token.tag_ in [\"PRP$\", \"POS\"]:\n",
    "        resolved[coref[0]] = mention_span.text + \"'s\" + final_token.whitespace_\n",
    "    else:\n",
    "        resolved[coref[0]] = mention_span.text + final_token.whitespace_\n",
    "    for i in range(coref[0] + 1, coref[1] + 1):\n",
    "        resolved[i] = \"\"\n",
    "    return resolved\n",
    "\n",
    "def get_span_noun_indices(doc: Doc, cluster: List[List[int]]) -> List[int]:\n",
    "    spans = [doc[span[0]:span[1]+1] for span in cluster]\n",
    "    spans_pos = [[token.pos_ for token in span] for span in spans]\n",
    "    span_noun_indices = [i for i, span_pos in enumerate(spans_pos)\n",
    "        if any(pos in span_pos for pos in ['NOUN', 'PROPN'])]\n",
    "    return span_noun_indices\n",
    "\n",
    "def get_cluster_head(doc: Doc, cluster: List[List[int]], noun_indices: List[int]):\n",
    "    head_idx = noun_indices[0]\n",
    "    head_start, head_end = cluster[head_idx]\n",
    "    head_span = doc[head_start:head_end+1]\n",
    "    return head_span, [head_start, head_end]\n",
    "\n",
    "def is_containing_other_spans(span: List[int], all_spans: List[List[int]]):\n",
    "    return any([s[0] >= span[0] and s[1] <= span[1] and s != span for s in all_spans])\n",
    "\n",
    "def improved_replace_corefs(document, clusters):\n",
    "    resolved = list(tok.text_with_ws for tok in document)\n",
    "    all_spans = [span for cluster in clusters for span in cluster]  # flattened list of all spans\n",
    "\n",
    "    for cluster in clusters:\n",
    "        noun_indices = get_span_noun_indices(document, cluster)\n",
    "\n",
    "        if noun_indices:\n",
    "            mention_span, mention = get_cluster_head(document, cluster, noun_indices)\n",
    "\n",
    "            for coref in cluster:\n",
    "                if coref != mention and not is_containing_other_spans(coref, all_spans):\n",
    "                    core_logic_part(document, coref, resolved, mention_span)\n",
    "\n",
    "    return \"\".join(resolved)\n",
    "\n",
    "def get_fast_cluster_spans(doc, clusters):\n",
    "    fast_clusters = []\n",
    "    for cluster in clusters:\n",
    "        new_group = []\n",
    "        for tuple in cluster:\n",
    "            (start, end) = tuple\n",
    "            span = doc.char_span(start, end)\n",
    "            new_group.append([span.start, span.end-1])\n",
    "        fast_clusters.append(new_group)\n",
    "    return fast_clusters\n",
    "\n",
    "def get_fastcoref_clusters(doc, text):\n",
    "    preds = model.predict(texts=[text])\n",
    "    fast_clusters = preds[0].get_clusters(as_strings=False)\n",
    "    fast_cluster_spans = get_fast_cluster_spans(doc, fast_clusters)\n",
    "    return fast_cluster_spans\n",
    "\n",
    "# def named_entity_linking(doc: Doc):\n",
    "#     entity_mentions = []\n",
    "#     for ent in doc.ents:\n",
    "#         entity_mentions.append((ent.text, ent.label_))  # Store (text, label) tuple\n",
    "#     return entity_mentions\n",
    "\n",
    "def get_entity_id(label: str, mention: str):\n",
    "    # Search Wikipedia for the entity using mention text\n",
    "    page = wiki_wiki.page(mention)\n",
    "    if page.exists():\n",
    "        return page.title, page.fullurl  # Return title and URL of the Wikipedia page\n",
    "    else:\n",
    "        return \"Unknown\", None  # Return placeholder if entity not found\n",
    "\n",
    "def link_named_entities(entity_mentions: List[Tuple[str, str]]):\n",
    "    linked_entities = set()  # Keep track of linked entities\n",
    "    for mention, label in entity_mentions:\n",
    "        if mention not in linked_entities:\n",
    "            entity_title, entity_url = get_entity_id(label, mention)\n",
    "            print(f\"{label}\\t{mention}\\t{entity_title}\\t{entity_url}\")\n",
    "            linked_entities.add(mention)  # Add linked entity to set\n",
    "\n",
    "\n",
    "def process_text(text: str):\n",
    "    doc = nlp(text)\n",
    "    clusters = get_fastcoref_clusters(doc, text)\n",
    "    coref_text = improved_replace_corefs(doc, clusters)\n",
    "    entity_mentions = named_entity_linking(doc)\n",
    "    link_named_entities(entity_mentions)\n",
    "    # print('coref_text', coref_text)\n",
    "\n",
    "def coreference(text: str) -> str:\n",
    "    doc = nlp(text)\n",
    "    clusters = get_fastcoref_clusters(doc, text)\n",
    "    coref_text = improved_replace_corefs(doc, clusters)\n",
    "    return coref_text\n",
    "def get_entity_mentions(doc):\n",
    "    entity_mentions = []\n",
    "    for ent in doc.ents:\n",
    "        entity_mentions.append((ent.text, ent.label_))\n",
    "    return entity_mentions\n",
    "\n",
    "def get_entity_id(label, mention):\n",
    "    # Search Wikipedia for the entity using mention text\n",
    "    page = wiki_wiki.page(mention)\n",
    "    if page.exists():\n",
    "        return page.title, page.fullurl  # Return title and URL of the Wikipedia page\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def named_entity_linking(resolved_text):\n",
    "    doc = nlp(resolved_text)\n",
    "    entity_mentions = get_entity_mentions(doc)\n",
    "    linked_entities = set()\n",
    "    linked_entities_list = []\n",
    "    for mention, label in entity_mentions:\n",
    "        if mention not in linked_entities:\n",
    "            entity_title, entity_url = get_entity_id(label, mention)\n",
    "            if entity_title and entity_url:\n",
    "                print(f\"{label}\\t{mention}\\t{entity_title}\\t{entity_url}\")\n",
    "            linked_entities.add(mention)\n",
    "            linked_entities_list.append(doc)  # Append the spaCy Doc object\n",
    "    return linked_entities_list\n",
    "\n",
    "\n",
    "def save_linked_entities_to_csv(linked_entities_list):\n",
    "    with open(\"linked_entities.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow([\"Label\", \"Mention\", \"Entity Title\", \"Entity URL\"])\n",
    "        csv_writer.writerows(linked_entities_list)\n",
    "\n",
    "patterns = [\n",
    "    [{\"DEP\": \"nsubjpass\", \"OP\": \"?\"}],  # Optional passive subject\n",
    "    [{\"DEP\": {\"in\": [\"agent\", \"pobj\"]}}, {\"OP\": \"?\"}],  # Optional agent or passive object\n",
    "    [{\"DEP\": \"prep\"}, {\"OP\": \"?\"}],  # Optional preposition\n",
    "    [{\"DEP\": {\"in\": [\"attr\", \"dobj\", \"pobj\"]}}],  # Direct object or prepositional object\n",
    "]\n",
    "\n",
    "# Initialize Matcher with patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('RELATION', patterns)\n",
    "\n",
    "\n",
    "def extract_relationships(doc: Doc):\n",
    "    \"\"\"Extract relationships between entities in the document.\"\"\"\n",
    "    relationships = []\n",
    "    for ent1 in doc.ents:\n",
    "        for ent2 in doc.ents:\n",
    "            if ent1 != ent2:\n",
    "                # Check if ent1 is before ent2 in the document\n",
    "                if ent1.start < ent2.start:\n",
    "                    source = ent1.text\n",
    "                    target = ent2.text\n",
    "                else:\n",
    "                    source = ent2.text\n",
    "                    target = ent1.text\n",
    "                # Define the type of relationship based on the entity labels\n",
    "                relation_type = determine_relation_type(ent1.label_, ent2.label_)\n",
    "                relationships.append((source, target, relation_type))\n",
    "    return relationships\n",
    "def abh_toh_maja_aayega(entities)\n",
    "    for permutation in itertools.permutations(entities, 2):\n",
    "        source_characters = ''.join(token.text_with_ws for token in permutation[0])\n",
    "        target_characters = ''.join(token.text_with_ws for token in permutation[1])\n",
    "        # Relationship extraction with OpenNRE\n",
    "        data = relation_model.infer(\n",
    "            {'text': resolved_text, 'h': {'pos': [permutation[0].start_char, permutation[0].end_char]}, 't': {'pos': [permutation[1].start_char, permutation[1].end_char]}})\n",
    "        if data[1] > relation_threshold:\n",
    "            relations_list.append(\n",
    "                {'source': source_characters, 'target': target_characters, 'type': data[0]})\n",
    "            \n",
    "def determine_relation_type(label1: str, label2: str) -> str:\n",
    "    \"\"\"Determine the type of relationship between two entity labels.\"\"\"\n",
    "    if label1 == 'ORG' and label2 == 'PERSON':\n",
    "        return 'owned by'\n",
    "    elif label1 == 'PERSON' and label2 == 'ORG':\n",
    "        return 'founded by'\n",
    "    elif label1 == 'PERSON' and label2 == 'ORG':\n",
    "        return 'founded by'\n",
    "    elif label1 == 'ORG' and label2 == 'ORG':\n",
    "        return 'partnership'\n",
    "    elif label1 == 'PERSON' and label2 == 'PERSON':\n",
    "        return 'colleagues'\n",
    "    elif label1 == 'PERSON' and label2 == 'GPE':\n",
    "        return 'work location'\n",
    "    elif label1 == 'GPE' and label2 == 'PERSON':\n",
    "        return 'residence'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = \"Elon Musk is a business magnate, industrial designer, and engineer. He is the founder, CEO, CTO, and chief designer of SpaceX. He is also early investor, CEO, and product architect of Tesla, Inc. He is also the founder of The Boring Company and the co-founder of Neuralink. A centibillionaire, Musk became the richest person in the world in January 2021, with an estimated net worth of $185 billion at the time, surpassing Jeff Bezos. Musk was born to a Canadian mother and South African father and raised in Pretoria, South Africa. He briefly attended the University of Pretoria before moving to Canada aged 17 to attend Queen's University. He transferred to the University of Pennsylvania two years later, where he received dual bachelor's degrees in economics and physics. He moved to California in 1995 to attend Stanford University, but decided instead to pursue a business career. He went on co-founding a web software company Zip2 with his brother Kimbal Musk.\"\n",
    "    resolved_text = coreference(text)\n",
    "    entities = named_entity_linking(resolved_text)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
