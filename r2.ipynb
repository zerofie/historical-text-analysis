{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.0.3)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flask) (3.0.2)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flask) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flask) (2.1.2)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flask) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flask) (1.7.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Jinja2>=3.1.2->flask) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install flask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (24.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (69.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip setuptools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following yanked versions: 1.11.0\n",
      "ERROR: Ignored the following versions that require a different python version: 1.10.0 Requires-Python <3.12,>=3.8; 1.10.0rc1 Requires-Python <3.12,>=3.8; 1.10.0rc2 Requires-Python <3.12,>=3.8; 1.10.1 Requires-Python <3.12,>=3.8; 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10; 1.7.2 Requires-Python >=3.7,<3.11; 1.7.3 Requires-Python >=3.7,<3.11; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11; 1.9.0 Requires-Python >=3.8,<3.12; 1.9.0rc1 Requires-Python >=3.8,<3.12; 1.9.0rc2 Requires-Python >=3.8,<3.12; 1.9.0rc3 Requires-Python >=3.8,<3.12; 1.9.1 Requires-Python >=3.8,<3.12\n",
      "ERROR: Could not find a version that satisfies the requirement scipy==1.7.3 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0, 1.4.1, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0, 1.6.1, 1.9.2, 1.9.3, 1.11.0rc1, 1.11.0rc2, 1.11.1, 1.11.2, 1.11.3, 1.11.4, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.13.0rc1, 1.13.0)\n",
      "ERROR: No matching distribution found for scipy==1.7.3\n"
     ]
    }
   ],
   "source": [
    "pip install scipy==1.7.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.2.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 1)) (2.2.2+cu118)\n",
      "Collecting transformers==3.4.0 (from -r requirements.txt (line 2))\n",
      "  Using cached transformers-3.4.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting pytest==5.3.2 (from -r requirements.txt (line 3))\n",
      "  Using cached pytest-5.3.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting scikit-learn==0.22.1 (from -r requirements.txt (line 4))\n",
      "  Using cached scikit-learn-0.22.1.tar.gz (6.9 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: scipy>1.4.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 5)) (1.12.0)\n",
      "Requirement already satisfied: nltk>=3.6.4 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 6)) (3.8.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (2024.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (1.26.4)\n",
      "Collecting tokenizers==0.9.2 (from transformers==3.4.0->-r requirements.txt (line 2))\n",
      "  Using cached tokenizers-0.9.2.tar.gz (170 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (4.66.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (2023.12.25)\n",
      "Collecting sentencepiece!=0.1.92 (from transformers==3.4.0->-r requirements.txt (line 2))\n",
      "  Using cached sentencepiece-0.2.0-cp312-cp312-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting protobuf (from transformers==3.4.0->-r requirements.txt (line 2))\n",
      "  Using cached protobuf-5.26.1-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting sacremoses (from transformers==3.4.0->-r requirements.txt (line 2))\n",
      "  Using cached sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting py>=1.5.0 (from pytest==5.3.2->-r requirements.txt (line 3))\n",
      "  Using cached py-1.11.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (23.2.0)\n",
      "Collecting more-itertools>=4.0.0 (from pytest==5.3.2->-r requirements.txt (line 3))\n",
      "  Using cached more_itertools-10.2.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting pluggy<1.0,>=0.12 (from pytest==5.3.2->-r requirements.txt (line 3))\n",
      "  Using cached pluggy-0.13.1-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (0.2.13)\n",
      "Collecting atomicwrites>=1.0 (from pytest==5.3.2->-r requirements.txt (line 3))\n",
      "  Using cached atomicwrites-1.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn==0.22.1->-r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.6.4->-r requirements.txt (line 6)) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch==2.2.2->-r requirements.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers==3.4.0->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers==3.4.0->-r requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers==3.4.0->-r requirements.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers==3.4.0->-r requirements.txt (line 2)) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch==2.2.2->-r requirements.txt (line 1)) (1.3.0)\n",
      "Using cached transformers-3.4.0-py3-none-any.whl (1.3 MB)\n",
      "Using cached pytest-5.3.2-py3-none-any.whl (234 kB)\n",
      "Using cached more_itertools-10.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
      "Using cached py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "Using cached sentencepiece-0.2.0-cp312-cp312-win_amd64.whl (991 kB)\n",
      "Using cached protobuf-5.26.1-cp310-abi3-win_amd64.whl (420 kB)\n",
      "Using cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "Building wheels for collected packages: scikit-learn, tokenizers\n",
      "  Building wheel for scikit-learn (pyproject.toml): started\n",
      "  Building wheel for scikit-learn (pyproject.toml): finished with status 'error'\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Failed to build scikit-learn tokenizers\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for scikit-learn (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [36 lines of output]\n",
      "      <string>:12: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "      Partial import of sklearn during the build process.\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 195, in check_package_status\n",
      "        File \"c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "          return _bootstrap._gcd_import(name[level:], package, level)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "        File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "        File \"<frozen importlib._bootstrap>\", line 1324, in _find_and_load_unlocked\n",
      "      ModuleNotFoundError: No module named 'numpy'\n",
      "      Traceback (most recent call last):\n",
      "        File \"c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 251, in build_wheel\n",
      "          return _build_backend().build_wheel(wheel_directory, config_settings,\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-build-env-cbzqaask\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 410, in build_wheel\n",
      "          return self._build_with_temp_dir(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-build-env-cbzqaask\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 395, in _build_with_temp_dir\n",
      "          self.run_setup()\n",
      "        File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-build-env-cbzqaask\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 487, in run_setup\n",
      "          super().run_setup(setup_script=setup_script)\n",
      "        File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-build-env-cbzqaask\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 311, in run_setup\n",
      "          exec(code, locals())\n",
      "        File \"<string>\", line 303, in <module>\n",
      "        File \"<string>\", line 291, in setup_package\n",
      "        File \"<string>\", line 219, in check_package_status\n",
      "      ImportError: numpy is not installed.\n",
      "      scikit-learn requires numpy >= 1.11.0.\n",
      "      Installation instructions are available on the scikit-learn website: http://scikit-learn.org/stable/install.html\n",
      "      \n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for scikit-learn\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [47 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-312\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\n",
      "      copying py_src\\tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\models\n",
      "      copying py_src\\tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\models\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\decoders\n",
      "      copying py_src\\tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\decoders\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\normalizers\n",
      "      copying py_src\\tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\normalizers\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\pre_tokenizers\n",
      "      copying py_src\\tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\pre_tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\processors\n",
      "      copying py_src\\tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\processors\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\trainers\n",
      "      copying py_src\\tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\trainers\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\sentencepiece_unigram.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\n",
      "      copying py_src\\tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\models\n",
      "      copying py_src\\tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\decoders\n",
      "      copying py_src\\tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\normalizers\n",
      "      copying py_src\\tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\pre_tokenizers\n",
      "      copying py_src\\tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\processors\n",
      "      copying py_src\\tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\trainers\n",
      "      running build_ext\n",
      "      running build_rust\n",
      "      error: can't find Rust compiler\n",
      "      \n",
      "      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "      \n",
      "      To update pip, run:\n",
      "      \n",
      "          pip install --upgrade pip\n",
      "      \n",
      "      and then retry package installation.\n",
      "      \n",
      "      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for scikit-learn, tokenizers, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.17.2+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"setup.py\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip setup.py install \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"setup.py\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip setup.py develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import urllib\n",
    "import opennre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 00:26:02,688 - datasets - INFO - PyTorch version 2.2.2+cu118 available.\n",
      "2024-04-14 00:26:05,754 - fastcoref.modeling - INFO - missing_keys: []\n",
      "2024-04-14 00:26:05,755 - fastcoref.modeling - INFO - unexpected_keys: []\n",
      "2024-04-14 00:26:05,756 - fastcoref.modeling - INFO - mismatched_keys: []\n",
      "2024-04-14 00:26:05,756 - fastcoref.modeling - INFO - error_msgs: []\n",
      "2024-04-14 00:26:05,759 - fastcoref.modeling - INFO - Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from typing import List\n",
    "from spacy.tokens import Doc, Span\n",
    "from fastcoref import FCoref\n",
    "#from fastcoref import LingMessCoref\n",
    "# from flask import Flask, render_template, request, redirect, session, jsonify\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "model = FCoref()\n",
    "#model = LingMessCoref(device='cuda:0')\n",
    "# app = Flask(__name__)\n",
    "\n",
    "def core_logic_part(document: Doc, coref: List[int], resolved: List[str], mention_span: Span):\n",
    "    final_token = document[coref[1]]\n",
    "    if final_token.tag_ in [\"PRP$\", \"POS\"]:\n",
    "        resolved[coref[0]] = mention_span.text + \"'s\" + final_token.whitespace_\n",
    "    else:\n",
    "        resolved[coref[0]] = mention_span.text + final_token.whitespace_\n",
    "    for i in range(coref[0] + 1, coref[1] + 1):\n",
    "        resolved[i] = \"\"\n",
    "    return resolved\n",
    "\n",
    "def get_span_noun_indices(doc: Doc, cluster: List[List[int]]) -> List[int]:\n",
    "    spans = [doc[span[0]:span[1]+1] for span in cluster]\n",
    "    spans_pos = [[token.pos_ for token in span] for span in spans]\n",
    "    span_noun_indices = [i for i, span_pos in enumerate(spans_pos)\n",
    "        if any(pos in span_pos for pos in ['NOUN', 'PROPN'])]\n",
    "    return span_noun_indices\n",
    "\n",
    "def get_cluster_head(doc: Doc, cluster: List[List[int]], noun_indices: List[int]):\n",
    "    head_idx = noun_indices[0]\n",
    "    head_start, head_end = cluster[head_idx]\n",
    "    head_span = doc[head_start:head_end+1]\n",
    "    return head_span, [head_start, head_end]\n",
    "\n",
    "def is_containing_other_spans(span: List[int], all_spans: List[List[int]]):\n",
    "    return any([s[0] >= span[0] and s[1] <= span[1] and s != span for s in all_spans])\n",
    "\n",
    "def improved_replace_corefs(document, clusters):\n",
    "    resolved = list(tok.text_with_ws for tok in document)\n",
    "    all_spans = [span for cluster in clusters for span in cluster]  # flattened list of all spans\n",
    "\n",
    "    for cluster in clusters:\n",
    "        noun_indices = get_span_noun_indices(document, cluster)\n",
    "\n",
    "        if noun_indices:\n",
    "            mention_span, mention = get_cluster_head(document, cluster, noun_indices)\n",
    "\n",
    "            for coref in cluster:\n",
    "                if coref != mention and not is_containing_other_spans(coref, all_spans):\n",
    "                    core_logic_part(document, coref, resolved, mention_span)\n",
    "\n",
    "    return \"\".join(resolved)\n",
    "\n",
    "def get_fast_cluster_spans(doc, clusters):\n",
    "    fast_clusters = []\n",
    "    for cluster in clusters:\n",
    "        new_group = []\n",
    "        for tuple in cluster:\n",
    "            print(type(tuple), tuple)\n",
    "            (start, end) = tuple\n",
    "            print(\"start, end\", start, end)\n",
    "            span = doc.char_span(start, end)\n",
    "            print('span', span.start, span.end)\n",
    "            new_group.append([span.start, span.end-1])\n",
    "        fast_clusters.append(new_group)\n",
    "    return fast_clusters\n",
    "\n",
    "def get_fastcoref_clusters(doc, text):\n",
    "    preds = model.predict(texts=[text])\n",
    "    fast_clusters = preds[0].get_clusters(as_strings=False)\n",
    "    fast_cluster_spans = get_fast_cluster_spans(doc, fast_clusters)\n",
    "    return fast_cluster_spans\n",
    "\n",
    "# @app.route('/coreference', methods=['POST'])\n",
    "def coreference(text):\n",
    "    #print(request.form[\"test\"])\n",
    "    # content = request.get_json()\n",
    "    # print('content', content)\n",
    "    # text = content[\"text\"]\n",
    "   \n",
    "    doc = nlp(text)\n",
    "    #clusters = get_allennlp_clusters(text)\n",
    "    clusters = get_fastcoref_clusters(doc, text)\n",
    "    coref_text = improved_replace_corefs(doc, clusters)\n",
    "    print('coref_text', coref_text)\n",
    "    return coref_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "from string import punctuation\n",
    "import nltk\n",
    "\n",
    "# Rest of your code...\n",
    "\n",
    "\n",
    "ENTITY_TYPES = [\"human\", \"person\", \"company\", \"enterprise\", \"business\", \"geographic region\",\n",
    "                \"human settlement\", \"geographic entity\", \"territorial entity type\", \"organization\"]\n",
    "\n",
    "def wikifier(text, lang=\"en\", threshold=0.8):\n",
    "    \"\"\"Function that fetches entity linking results from wikifier.com API\"\"\"\n",
    "    # Prepare the URL.\n",
    "    data = urllib.parse.urlencode([\n",
    "        (\"text\", text), (\"lang\", lang),\n",
    "        (\"userKey\", \"tgbdmkpmkluegqfbawcwjywieevmza\"),\n",
    "        (\"pageRankSqThreshold\", \"%g\" %\n",
    "         threshold), (\"applyPageRankSqThreshold\", \"true\"),\n",
    "        (\"nTopDfValuesToIgnore\", \"100\"), (\"nWordsToIgnoreFromList\", \"100\"),\n",
    "        (\"wikiDataClasses\", \"true\"), (\"wikiDataClassIds\", \"false\"),\n",
    "        (\"support\", \"true\"), (\"ranges\", \"false\"), (\"minLinkFrequency\", \"2\"),\n",
    "        (\"includeCosines\", \"false\"), (\"maxMentionEntropy\", \"3\")\n",
    "    ])\n",
    "    url = \"http://www.wikifier.org/annotate-article\"\n",
    "    # Call the Wikifier and read the response.\n",
    "    req = urllib.request.Request(url, data=data.encode(\"utf8\"), method=\"POST\")\n",
    "    with urllib.request.urlopen(req, timeout=60) as f:\n",
    "        response = f.read()\n",
    "        response = json.loads(response.decode(\"utf8\"))\n",
    "    # Output the annotations.\n",
    "    results = list()\n",
    "    for annotation in response[\"annotations\"]:\n",
    "        # Filter out desired entity classes\n",
    "        if ('wikiDataClasses' in annotation) and (any([el['enLabel'] in ENTITY_TYPES for el in annotation['wikiDataClasses']])):\n",
    "\n",
    "            # Specify entity label\n",
    "            if any([el['enLabel'] in [\"human\", \"person\"] for el in annotation['wikiDataClasses']]):\n",
    "                label = 'Person'\n",
    "            elif any([el['enLabel'] in [\"company\", \"enterprise\", \"business\", \"organization\"] for el in annotation['wikiDataClasses']]):\n",
    "                label = 'Organization'\n",
    "            elif any([el['enLabel'] in [\"geographic region\", \"human settlement\", \"geographic entity\", \"territorial entity type\"] for el in annotation['wikiDataClasses']]):\n",
    "                label = 'Location'\n",
    "            else:\n",
    "                label = None\n",
    "\n",
    "            results.append({'title': annotation['title'], 'wikiId': annotation['wikiDataItemId'], 'label': label,\n",
    "                            'characters': [(el['chFrom'], el['chTo']) for el in annotation['support']]})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from flask import Flask, request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2024-04-14 01:25:18,789 - root - INFO - Initializing word embedding with word2vec.\n",
      "2024-04-14 01:25:18,924 - fastcoref.modeling - INFO - Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715c836067fe4406ba0c912d30256272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 01:25:19,027 - fastcoref.modeling - INFO - ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e5ae925b28428ebe2d8d5b8e95432a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> (146, 148)\n",
      "start, end 146 148\n",
      "span 34 35\n",
      "<class 'tuple'> (184, 186)\n",
      "start, end 184 186\n",
      "span 44 45\n",
      "<class 'tuple'> (230, 235)\n",
      "start, end 230 235\n",
      "span 54 55\n",
      "<class 'tuple'> (290, 293)\n",
      "start, end 290 293\n",
      "span 68 69\n",
      "<class 'tuple'> (307, 312)\n",
      "start, end 307 312\n",
      "span 72 73\n",
      "<class 'tuple'> (368, 373)\n",
      "start, end 368 373\n",
      "span 85 86\n",
      "<class 'tuple'> (434, 439)\n",
      "start, end 434 439\n",
      "span 97 98\n",
      "<class 'tuple'> (541, 543)\n",
      "start, end 541 543\n",
      "span 120 121\n",
      "<class 'tuple'> (609, 611)\n",
      "start, end 609 611\n",
      "span 134 135\n",
      "<class 'tuple'> (666, 668)\n",
      "start, end 666 668\n",
      "span 144 145\n",
      "<class 'tuple'> (715, 718)\n",
      "start, end 715 718\n",
      "span 153 154\n",
      "<class 'tuple'> (728, 730)\n",
      "start, end 728 730\n",
      "span 157 158\n",
      "<class 'tuple'> (794, 799)\n",
      "start, end 794 799\n",
      "span 169 170\n",
      "<class 'tuple'> (202, 228)\n",
      "start, end 202 228\n",
      "span 49 53\n",
      "<class 'tuple'> (290, 300)\n",
      "start, end 290 300\n",
      "span 68 70\n",
      "<class 'tuple'> (339, 350)\n",
      "start, end 339 350\n",
      "span 80 82\n",
      "<class 'tuple'> (565, 576)\n",
      "start, end 565 576\n",
      "span 125 127\n",
      "<class 'tuple'> (810, 827)\n",
      "start, end 810 827\n",
      "span 172 175\n",
      "<class 'tuple'> (854, 856)\n",
      "start, end 854 856\n",
      "span 181 182\n",
      "coref_text Akbar (Abu'l-Fath Jalal ud-b din Muhammad Akbar, 25 October 1542 – 27 October 1605), also known as Akbar the Great was the 3rd Mughal Emperor.[1] Akbar was born in Lahore (now Pakistan). Akbar was the son of 2nd Mughal Emperor Humayun. Akbar became the de jure king in 1556 at the age of 13 when Akbar's father died. Akbar was too young to rule, so Bairam Khan was appointed as Akbar regent and chief army commander. Soon after coming to power Akbar defeated Himu, the general of the Afghan forces, in the Second Battle of Panipat. After a few years, Akbar ended the regency of Bairam Khan and took charge of the kingdom. Akbar initially offered friendship to the Rajputs. However, Akbar had to fight against some Rajputs who opposed Akbar. In 1576 Akbar defeated Maharana Pratap of Mewar in the Battle of Haldighati. Akbar wars made the Mughal empire more than twice as big as the Mughal empire had been before, covering most of the Indian subcontinent except the south (excluding the Deccan Plateau)\n",
      "Akbar (Abu'l-Fath Jalal ud-b din Muhammad Akbar, 25 October 1542 – 27 October 1605), also known as Akbar the Great was the 3rd Mughal Emperor.[1] Akbar was born in Lahore (now Pakistan). Akbar was the son of 2nd Mughal Emperor Humayun. Akbar became the de jure king in 1556 at the age of 13 when Akbar's father died. Akbar was too young to rule, so Bairam Khan was appointed as Akbar regent and chief army commander. Soon after coming to power Akbar defeated Himu, the general of the Afghan forces, in the Second Battle of Panipat. After a few years, Akbar ended the regency of Bairam Khan and took charge of the kingdom. Akbar initially offered friendship to the Rajputs. However, Akbar had to fight against some Rajputs who opposed Akbar. In 1576 Akbar defeated Maharana Pratap of Mewar in the Battle of Haldighati. Akbar wars made the Mughal empire more than twice as big as the Mughal empire had been before, covering most of the Indian subcontinent except the south (excluding the Deccan Plateau)\n",
      "Entities:\n",
      "Akbar: Person\n",
      "Lahore: Organization\n",
      "Mughal Empire: Organization\n",
      "Humayun: Person\n",
      "Akbar: Person\n",
      "Akbar: Person\n",
      "Bairam Khan: Person\n",
      "Akbar: Person\n",
      "Panipat: Organization\n",
      "Akbar: Person\n",
      "Bairam Khan: Person\n",
      "Akbar: Person\n",
      "Maharana Pratap: Person\n",
      "Mewar: Organization\n",
      "Mughal Empire: Organization\n",
      "Deccan Plateau: Location\n",
      "\n",
      "Relations:\n",
      "Humayun -- position held --> Mughal Empire\n",
      "Humayun -- position held --> Mughal Empire\n",
      "Panipat -- participant --> Akbar\n"
     ]
    }
   ],
   "source": [
    "from opennre.pretrain import get_model\n",
    "import itertools\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "relation_model = get_model('wiki80_cnn_softmax')\n",
    "ENTITY_TYPES = [\"human\", \"person\", \"company\", \"enterprise\", \"business\", \"geographic region\",\n",
    "                \"human settlement\", \"geographic entity\", \"territorial entity type\", \"organization\"]\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    \"\"\"Removes all punctuation from a string\"\"\"\n",
    "    return ''.join(c for c in s if c not in punctuation)\n",
    "\n",
    "\n",
    "def deduplicate_dict(d):\n",
    "    return [dict(y) for y in set(tuple(x.items()) for x in d)]\n",
    "\n",
    "\n",
    "def final(text):\n",
    "   \n",
    "        relation_threshold=0.9\n",
    "        entities_threshold=0.8\n",
    "        coref =  True\n",
    "        if not text:\n",
    "            return 'Missing text parameter'\n",
    "\n",
    "        try:\n",
    "            relation_threshold = float(relation_threshold)\n",
    "            entities_threshold = float(entities_threshold)\n",
    "        except ValueError:\n",
    "            return 'Invalid value for relation or entity threshold parameter'\n",
    "\n",
    "        if coref:\n",
    "            text = coreference(text)\n",
    "\n",
    "        print(text)\n",
    "\n",
    "        relations_list = list()\n",
    "        entities_list = list()\n",
    "\n",
    "        for sentence in nltk.sent_tokenize(text):\n",
    "            sentence = strip_punctuation(sentence)\n",
    "            entities = wikifier(sentence, threshold=entities_threshold)\n",
    "            entities_list.extend(\n",
    "                [{'title': el['title'], 'wikiId': el['wikiId'], 'label': el['label']} for el in entities])\n",
    "            # Iterate over every permutation pair of entities\n",
    "            for permutation in itertools.permutations(entities, 2):\n",
    "                for source in permutation[0]['characters']:\n",
    "                    for target in permutation[1]['characters']:\n",
    "                        # Relationship extraction with OpenNRE\n",
    "                        data = relation_model.infer(\n",
    "                            {'text': sentence, 'h': {'pos': [source[0], source[1] + 1]}, 't': {'pos': [target[0], target[1] + 1]}})\n",
    "                        if data[1] > relation_threshold:\n",
    "                            relations_list.append(\n",
    "                                {'source': permutation[0]['title'], 'target': permutation[1]['title'], 'type': data[0]})\n",
    "        result = {'entities': deduplicate_dict(entities_list), 'relations': deduplicate_dict(relations_list)}\n",
    "\n",
    "        print(\"Entities:\")\n",
    "        for entity in entities_list:\n",
    "            print(f\"{entity['title']}: {entity['label']}\")\n",
    "\n",
    "        # Print relations for current sentence\n",
    "        print(\"\\nRelations:\")\n",
    "        for relation in relations_list:\n",
    "            print(f\"{relation['source']} -- {relation['type']} --> {relation['target']}\")\n",
    "    \n",
    "\n",
    "text= \"Akbar (Abu'l-Fath Jalal ud-b din Muhammad Akbar, 25 October 1542 – 27 October 1605), also known as Akbar the Great was the 3rd Mughal Emperor.[1] He was born in Lahore (now Pakistan). He was the son of 2nd Mughal Emperor Humayun. Akbar became the de jure king in 1556 at the age of 13 when his father died. Akbar was too young to rule, so Bairam Khan was appointed as Akbar regent and chief army commander. Soon after coming to power Akbar defeated Himu, the general of the Afghan forces, in the Second Battle of Panipat. After a few years, he ended the regency of Bairam Khan and took charge of the kingdom. He initially offered friendship to the Rajputs. However, he had to fight against some Rajputs who opposed him. In 1576 he defeated Maharana Pratap of Mewar in the Battle of Haldighati. Akbar wars made the Mughal empire more than twice as big as it had been before, covering most of the Indian subcontinent except the south (excluding the Deccan Plateau)\"\n",
    "# ctext=coreference(text)\n",
    "final(text)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=coreference()\n",
    "results=wikifier(text)\n",
    "# First get all the entities in the sentence\n",
    "entities = wikifier(sentence, threshold=entities_threshold)\n",
    "# Iterate over every permutation pair of entities\n",
    "for permutation in itertools.permutations(entities, 2):\n",
    "    for source in permutation[0]['characters']:\n",
    "        for target in permutation[1]['characters']:\n",
    "            # Relationship extraction with OpenNRE\n",
    "            data = relation_model.infer(\n",
    "                {'text': sentence, 'h': {'pos': [source[0], source[1] + 1]}, 't': {'pos': [target[0], target[1] + 1]}})\n",
    "            if data[1] > relation_threshold:\n",
    "                relations_list.append(\n",
    "                    {'source': permutation[0]['title'], 'target': permutation[1]['title'], 'type': data[0]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
