{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install flask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip setuptools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scipy==1.7.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip setup.py install \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip setup.py develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import urllib\n",
    "import opennre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from typing import List\n",
    "from spacy.tokens import Doc, Span\n",
    "from fastcoref import FCoref\n",
    "#from fastcoref import LingMessCoref\n",
    "# from flask import Flask, render_template, request, redirect, session, jsonify\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "model = FCoref()\n",
    "#model = LingMessCoref(device='cuda:0')\n",
    "# app = Flask(__name__)\n",
    "\n",
    "def core_logic_part(document: Doc, coref: List[int], resolved: List[str], mention_span: Span):\n",
    "    final_token = document[coref[1]]\n",
    "    if final_token.tag_ in [\"PRP$\", \"POS\"]:\n",
    "        resolved[coref[0]] = mention_span.text + \"'s\" + final_token.whitespace_\n",
    "    else:\n",
    "        resolved[coref[0]] = mention_span.text + final_token.whitespace_\n",
    "    for i in range(coref[0] + 1, coref[1] + 1):\n",
    "        resolved[i] = \"\"\n",
    "    return resolved\n",
    "\n",
    "def get_span_noun_indices(doc: Doc, cluster: List[List[int]]) -> List[int]:\n",
    "    spans = [doc[span[0]:span[1]+1] for span in cluster]\n",
    "    spans_pos = [[token.pos_ for token in span] for span in spans]\n",
    "    span_noun_indices = [i for i, span_pos in enumerate(spans_pos)\n",
    "        if any(pos in span_pos for pos in ['NOUN', 'PROPN'])]\n",
    "    return span_noun_indices\n",
    "\n",
    "def get_cluster_head(doc: Doc, cluster: List[List[int]], noun_indices: List[int]):\n",
    "    head_idx = noun_indices[0]\n",
    "    head_start, head_end = cluster[head_idx]\n",
    "    head_span = doc[head_start:head_end+1]\n",
    "    return head_span, [head_start, head_end]\n",
    "\n",
    "def is_containing_other_spans(span: List[int], all_spans: List[List[int]]):\n",
    "    return any([s[0] >= span[0] and s[1] <= span[1] and s != span for s in all_spans])\n",
    "\n",
    "def improved_replace_corefs(document, clusters):\n",
    "    resolved = list(tok.text_with_ws for tok in document)\n",
    "    all_spans = [span for cluster in clusters for span in cluster]  # flattened list of all spans\n",
    "\n",
    "    for cluster in clusters:\n",
    "        noun_indices = get_span_noun_indices(document, cluster)\n",
    "\n",
    "        if noun_indices:\n",
    "            mention_span, mention = get_cluster_head(document, cluster, noun_indices)\n",
    "\n",
    "            for coref in cluster:\n",
    "                if coref != mention and not is_containing_other_spans(coref, all_spans):\n",
    "                    core_logic_part(document, coref, resolved, mention_span)\n",
    "\n",
    "    return \"\".join(resolved)\n",
    "\n",
    "def get_fast_cluster_spans(doc, clusters):\n",
    "    fast_clusters = []\n",
    "    for cluster in clusters:\n",
    "        new_group = []\n",
    "        for tuple in cluster:\n",
    "            print(type(tuple), tuple)\n",
    "            (start, end) = tuple\n",
    "            print(\"start, end\", start, end)\n",
    "            span = doc.char_span(start, end)\n",
    "            print('span', span.start, span.end)\n",
    "            new_group.append([span.start, span.end-1])\n",
    "        fast_clusters.append(new_group)\n",
    "    return fast_clusters\n",
    "\n",
    "def get_fastcoref_clusters(doc, text):\n",
    "    preds = model.predict(texts=[text])\n",
    "    fast_clusters = preds[0].get_clusters(as_strings=False)\n",
    "    fast_cluster_spans = get_fast_cluster_spans(doc, fast_clusters)\n",
    "    return fast_cluster_spans\n",
    "\n",
    "# @app.route('/coreference', methods=['POST'])\n",
    "def coreference(text):\n",
    "    #print(request.form[\"test\"])\n",
    "    # content = request.get_json()\n",
    "    # print('content', content)\n",
    "    # text = content[\"text\"]\n",
    "   \n",
    "    doc = nlp(text)\n",
    "    #clusters = get_allennlp_clusters(text)\n",
    "    clusters = get_fastcoref_clusters(doc, text)\n",
    "    coref_text = improved_replace_corefs(doc, clusters)\n",
    "    print('coref_text', coref_text)\n",
    "    return coref_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "from string import punctuation\n",
    "import nltk\n",
    "\n",
    "# Rest of your code...\n",
    "\n",
    "\n",
    "ENTITY_TYPES = [\"human\", \"person\", \"company\", \"enterprise\", \"business\", \"geographic region\",\n",
    "                \"human settlement\", \"geographic entity\", \"territorial entity type\", \"organization\"]\n",
    "\n",
    "def wikifier(text, lang=\"en\", threshold=0.8):\n",
    "    \"\"\"Function that fetches entity linking results from wikifier.com API\"\"\"\n",
    "    # Prepare the URL.\n",
    "    data = urllib.parse.urlencode([\n",
    "        (\"text\", text), (\"lang\", lang),\n",
    "        (\"userKey\", \"tgbdmkpmkluegqfbawcwjywieevmza\"),\n",
    "        (\"pageRankSqThreshold\", \"%g\" %\n",
    "         threshold), (\"applyPageRankSqThreshold\", \"true\"),\n",
    "        (\"nTopDfValuesToIgnore\", \"100\"), (\"nWordsToIgnoreFromList\", \"100\"),\n",
    "        (\"wikiDataClasses\", \"true\"), (\"wikiDataClassIds\", \"false\"),\n",
    "        (\"support\", \"true\"), (\"ranges\", \"false\"), (\"minLinkFrequency\", \"2\"),\n",
    "        (\"includeCosines\", \"false\"), (\"maxMentionEntropy\", \"3\")\n",
    "    ])\n",
    "    url = \"http://www.wikifier.org/annotate-article\"\n",
    "    # Call the Wikifier and read the response.\n",
    "    req = urllib.request.Request(url, data=data.encode(\"utf8\"), method=\"POST\")\n",
    "    with urllib.request.urlopen(req, timeout=60) as f:\n",
    "        response = f.read()\n",
    "        response = json.loads(response.decode(\"utf8\"))\n",
    "    # Output the annotations.\n",
    "    results = list()\n",
    "    for annotation in response[\"annotations\"]:\n",
    "        # Filter out desired entity classes\n",
    "        if ('wikiDataClasses' in annotation) and (any([el['enLabel'] in ENTITY_TYPES for el in annotation['wikiDataClasses']])):\n",
    "\n",
    "            # Specify entity label\n",
    "            if any([el['enLabel'] in [\"human\", \"person\"] for el in annotation['wikiDataClasses']]):\n",
    "                label = 'Person'\n",
    "            elif any([el['enLabel'] in [\"company\", \"enterprise\", \"business\", \"organization\"] for el in annotation['wikiDataClasses']]):\n",
    "                label = 'Organization'\n",
    "            elif any([el['enLabel'] in [\"geographic region\", \"human settlement\", \"geographic entity\", \"territorial entity type\"] for el in annotation['wikiDataClasses']]):\n",
    "                label = 'Location'\n",
    "            else:\n",
    "                label = None\n",
    "\n",
    "            results.append({'title': annotation['title'], 'wikiId': annotation['wikiDataItemId'], 'label': label,\n",
    "                            'characters': [(el['chFrom'], el['chTo']) for el in annotation['support']]})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from flask import Flask, request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from opennre.pretrain import get_model\n",
    "import itertools\n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "relation_model = get_model('wiki80_cnn_softmax')\n",
    "ENTITY_TYPES = [\"human\", \"person\", \"company\", \"enterprise\", \"business\", \"geographic region\",\n",
    "                \"human settlement\", \"geographic entity\", \"territorial entity type\", \"organization\"]\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    \"\"\"Removes all punctuation from a string\"\"\"\n",
    "    return ''.join(c for c in s if c not in punctuation)\n",
    "\n",
    "\n",
    "def deduplicate_dict(d):\n",
    "    return [dict(y) for y in set(tuple(x.items()) for x in d)]\n",
    "\n",
    "\n",
    "def final(text):\n",
    "   \n",
    "        relation_threshold=0.9\n",
    "        entities_threshold=0.8\n",
    "        coref =  True\n",
    "        if not text:\n",
    "            return 'Missing text parameter'\n",
    "\n",
    "        try:\n",
    "            relation_threshold = float(relation_threshold)\n",
    "            entities_threshold = float(entities_threshold)\n",
    "        except ValueError:\n",
    "            return 'Invalid value for relation or entity threshold parameter'\n",
    "\n",
    "        if coref:\n",
    "            text = coreference(text)\n",
    "\n",
    "        print(text)\n",
    "\n",
    "        relations_list = list()\n",
    "        entities_list = list()\n",
    "\n",
    "        for sentence in nltk.sent_tokenize(text):\n",
    "            sentence = strip_punctuation(sentence)\n",
    "            entities = wikifier(sentence, threshold=entities_threshold)\n",
    "            entities_list.extend(\n",
    "                [{'title': el['title'], 'wikiId': el['wikiId'], 'label': el['label']} for el in entities])\n",
    "            # Iterate over every permutation pair of entities\n",
    "            for permutation in itertools.permutations(entities, 2):\n",
    "                for source in permutation[0]['characters']:\n",
    "                    for target in permutation[1]['characters']:\n",
    "                        # Relationship extraction with OpenNRE\n",
    "                        data = relation_model.infer(\n",
    "                            {'text': sentence, 'h': {'pos': [source[0], source[1] + 1]}, 't': {'pos': [target[0], target[1] + 1]}})\n",
    "                        if data[1] > relation_threshold:\n",
    "                            relations_list.append(\n",
    "                                {'source': permutation[0]['title'], 'target': permutation[1]['title'], 'type': data[0]})\n",
    "        result = {'entities': deduplicate_dict(entities_list), 'relations': deduplicate_dict(relations_list)}\n",
    "        with open('entities.csv', 'w', newline='', encoding='utf-8') as entities_file:\n",
    "            entities_writer = csv.DictWriter(entities_file, fieldnames=['title', 'wikiId', 'label'])\n",
    "            entities_writer.writeheader()\n",
    "            entities_writer.writerows(entities_list)\n",
    "\n",
    "        # Write relations to CSV\n",
    "        with open('relations.csv', 'w', newline='', encoding='utf-8') as relations_file:\n",
    "            relations_writer = csv.DictWriter(relations_file, fieldnames=['source', 'target', 'type'])\n",
    "            relations_writer.writeheader()\n",
    "            relations_writer.writerows(relations_list)\n",
    "        print(\"Entities:\")\n",
    "        for entity in entities_list:\n",
    "            print(f\"{entity['title']}: {entity['label']}\")\n",
    "\n",
    "        # Print relations for current sentence\n",
    "        print(\"\\nRelations:\")\n",
    "        for relation in relations_list:\n",
    "            print(f\"{relation['source']} -- {relation['type']} --> {relation['target']}\")\n",
    "    \n",
    "\n",
    "text= \"Akbar (Abu'l-Fath Jalal ud-b din Muhammad Akbar, 25 October 1542 â€“ 27 October 1605), also known as Akbar the Great was the 3rd Mughal Emperor.[1] He was born in Lahore (now Pakistan). He was the son of 2nd Mughal Emperor Humayun. Akbar became the de jure king in 1556 at the age of 13 when his father died. Akbar was too young to rule, so Bairam Khan was appointed as Akbar regent and chief army commander. Soon after coming to power Akbar defeated Himu, the general of the Afghan forces, in the Second Battle of Panipat. After a few years, he ended the regency of Bairam Khan and took charge of the kingdom. He initially offered friendship to the Rajputs. However, he had to fight against some Rajputs who opposed him. In 1576 he defeated Maharana Pratap of Mewar in the Battle of Haldighati. Akbar wars made the Mughal empire more than twice as big as it had been before, covering most of the Indian subcontinent except the south (excluding the Deccan Plateau)\"\n",
    "# ctext=coreference(text)\n",
    "final(text)\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
